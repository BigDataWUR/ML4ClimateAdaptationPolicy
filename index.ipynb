{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot Project: Learning the Climate\n",
    "\n",
    "Author: Shashi Badloe, Ioannis N. Athanasiadis\n",
    "\n",
    "\n",
    "Purpose: Provides a step-by-step interactive pipeline for the cleanup, modeling and storing of data used in paper:  \n",
    "Robbert Biesbroek, Shashi Badloe, Ioannis N. Athanasiadis (2020). [Machine learning for research on climate change adaptation policy integration: an exploratory UK case study](http://dx.doi.org/10.1007/s10113-020-01677-8), published in Regional Environmental Change.\n",
    "\n",
    "\n",
    "Note: It is important to not change any folder names or directories created during intermediate steps in the pipeline. Doing so, may raise errors as the scripts look in folders with these specific names in specified relative directories.\n",
    "\n",
    "\n",
    "For further details on the function of each script, please look into the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Defining input data\n",
    "To start the analysis, we need to define the training data and testing data. In the folder 'PDF_files' the PDF documents used as training or testing data are contained. \n",
    "\n",
    "The following four folders should be in `PDF_files`:\n",
    "- `Adaptation policy documents`: Training data for adaptation policies\n",
    "- `Mitigation policy documents`: Training data for mitigation policies\n",
    "- `Non-climate policy documents`: Training data for non-climate documents\n",
    "- `Mixed policy documents`: Testing data, any PDF document(s) you want to predict on.\n",
    "\n",
    "You may define your own training and testing data by placing the PDF files in the specified folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Turning PDF into raw text and translating into bag-of-words\n",
    "PDF files are inaccessible to the machine learning algorithm, therefore we need to extract the raw text from PDF files. Converting from PDF to raw text can alter/break the composition of the text (i.e. add invalid characters or whitespace between words). We have applied some automated checks and edits to fix the most common problems, but there may still be  files that do not translate into raw text properly.\n",
    "\n",
    "The following scripts are used to convert and clean the PDF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory to the folder that holds all scripts\n",
    "import os\n",
    "script_folder = 'Python Scripts'\n",
    "if not os.getcwd().endswith(script_folder):\n",
    "    os.chdir(script_folder)\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will run the following script to convert all PDFs into raw text.\n",
    "\n",
    "It attempts to preserve paragraph structure within text and fixes for invalid characters.\n",
    "\n",
    "This connects to a server for PDF parsing so internet connection is required. If connection fails try re-running this.\n",
    "\n",
    "Outputs folder is named [`parsed_files`](parsed_files), and holds raw text for every PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Run script to convert all PDFs into raw text.\n",
    "\n",
    "exec(open('pdf_parser.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to clean up the raw texts, and create eligible blocks based on paragraph size.\n",
    "\n",
    "The following script uses tagger to determine word types and filters for useful words\n",
    "\n",
    "Outputs folder named [`structured_files`](structured_files), and contains a bag-of-words for every file in python list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run script to clean up the raw texts, and create eligible blocks based on paragraph size.\n",
    "\n",
    "exec(open('text_cleanup.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Building the database\n",
    "\n",
    "A SQLite database is created to hold every block. This database allows for quick storing and retrieval of data and is required when working with big data.\n",
    "\n",
    "Note that this requires a supply of metadata in 'metadata.txt'\n",
    "The metadata should have a python dictionary format where the key is the filename and contents are a \n",
    "tuple of date and department like so: `pdf_filename.pdf: ('day month year', 'Department')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script to create database and lots training data in Labeled_data and testing data in Unlabeled_data tables.\n",
    "# Outputs 'climate.db' file in scripts folder. Will overwrite any file with the same name.\n",
    "\n",
    "exec(open('sqlite_db.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Training the model\n",
    "The model is a simple feed forward neural network model. \n",
    "\n",
    "The input are the blocks and it assigns/adjusts weights of each word towards the three classes depending on how often they occur in the training data for that class or how often they co-occur in the same bag as a word strongly correlated to a class.\n",
    "\n",
    "First, we build a vocabulary from training data. The output file ['conversion dictionary.txt'](conversion dictionary.txt) is a python dictionary where every word in the training corpus is assigned a number.\n",
    "\n",
    "Then, we generate the neural network and start training. The generated model is stored in a folder named 'tensorflow/logdir\n",
    "\n",
    "Finally, we use the stored model to predict on new data. Results stored in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script to build vocabulary from training data \n",
    "\n",
    "exec(open('numberizer.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this script to start training the neural network\n",
    "\n",
    "exec(open('TF_classification_BW.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this script to predict on unseen data\n",
    "#Predicts the class of blocks in 'Unlabeled_data' table in the climate.db database.\n",
    "exec(open('TF_classification_predict.py').read())\n",
    "\n",
    "# Prepare the visualizations\n",
    "exec(open('document_prediction.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is finished. \n",
    "The predictions for the 9 sample documents in the test folder [Mixed policy documents](PDF_files/Mitigation%20policy%20documents) are below.\n",
    "\n",
    "![img](Plots/doc_pred.png) \n",
    "![img](Plots/doc_prob.png) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
