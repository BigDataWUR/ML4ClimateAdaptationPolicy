{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot Project: Learning the Climate\n",
    "\n",
    "Author: Shashi Badloe, Ioannis N. Athanasiadis\n",
    "\n",
    "\n",
    "Purpose: Provides a step-by-step interactive pipeline for the cleanup, modeling and storing of data used in paper:  \n",
    "Robbert Biesbroek, Shashi Badloe, Ioannis N. Athanasiadis (2020). [Machine learning for research on climate change adaptation policy integration: an exploratory UK case study](http://dx.doi.org/10.1007/s10113-020-01677-8), published in Regional Environmental Change.\n",
    "\n",
    "\n",
    "Note: It is important to not change any folder names or directories created during intermediate steps in the pipeline. Doing so, may raise errors as the scripts look in folders with these specific names in specified relative directories.\n",
    "\n",
    "\n",
    "For further details on the function of each script, please look into the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Defining input data\n",
    "To start the analysis, we need to define the training data and testing data. In the folder 'PDF_files' the PDF documents used as training or testing data are contained. \n",
    "\n",
    "The following four folders should be in `PDF_files`:\n",
    "- `Adaptation policy documents`: Training data for adaptation policies\n",
    "- `Mitigation policy documents`: Training data for mitigation policies\n",
    "- `Non-climate policy documents`: Training data for non-climate documents\n",
    "- `Mixed policy documents`: Testing data, any PDF document(s) you want to predict on.\n",
    "\n",
    "You may define your own training and testing data by placing the PDF files in the specified folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Turning PDF into raw text and translating into bag-of-words\n",
    "PDF files are inaccessible to the machine learning algorithm, therefore we need to extract the raw text from PDF files. Converting from PDF to raw text can alter/break the composition of the text (i.e. add invalid characters or whitespace between words). We have applied some automated checks and edits to fix the most common problems, but there may still be  files that do not translate into raw text properly.\n",
    "\n",
    "The following scripts are used to convert and clean the PDF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory  /home/jovyan/Python Scripts\n"
     ]
    }
   ],
   "source": [
    "#Set working directory to the folder that holds all scripts\n",
    "import os\n",
    "script_folder = 'Python Scripts'\n",
    "if not os.getcwd().endswith(script_folder):\n",
    "    os.chdir(script_folder)\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will run the following script to convert all PDFs into raw text.\n",
    "\n",
    "It attempts to preserve paragraph structure within text and fixes for invalid characters.\n",
    "\n",
    "This connects to a server for PDF parsing so internet connection is required. If connection fails try re-running this.\n",
    "\n",
    "Outputs folder is named [`parsed_files`](parsed_files), and holds raw text for every PDF file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-06-28 19:25:13,640 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
      "2020-06-28 19:25:15,475 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
      "2020-06-28 19:25:15,961 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished parsing PDF files\n"
     ]
    }
   ],
   "source": [
    "#Run script to convert all PDFs into raw text.\n",
    "\n",
    "exec(open('pdf_parser.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our next step is to clean up the raw texts, and create eligible blocks based on paragraph size.\n",
    "\n",
    "The following script uses tagger to determine word types and filters for useful words\n",
    "\n",
    "Outputs folder named [`structured_files`](structured_files), and contains a bag-of-words for every file in python list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "Data took 81 seconds to structurize\n"
     ]
    }
   ],
   "source": [
    "#Run script to clean up the raw texts, and create eligible blocks based on paragraph size.\n",
    "\n",
    "exec(open('text_cleanup.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Building the database\n",
    "\n",
    "A SQLite database is created to hold every block. This database allows for quick storing and retrieval of data and is required when working with big data.\n",
    "\n",
    "Note that this requires a supply of metadata in 'metadata.txt'\n",
    "The metadata should have a python dictionary format where the key is the filename and contents are a \n",
    "tuple of date and department like so: `pdf_filename.pdf: ('day month year', 'Department')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database took 1 seconds to create\n"
     ]
    }
   ],
   "source": [
    "# Run script to create database and lots training data in Labeled_data and testing data in Unlabeled_data tables.\n",
    "# Outputs 'climate.db' file in scripts folder. Will overwrite any file with the same name.\n",
    "\n",
    "exec(open('sqlite_db.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Training the model\n",
    "The model is a simple feed forward neural network model. \n",
    "\n",
    "The input are the blocks and it assigns/adjusts weights of each word towards the three classes depending on how often they occur in the training data for that class or how often they co-occur in the same bag as a word strongly correlated to a class.\n",
    "\n",
    "First, we build a vocabulary from training data. The output file ['conversion dictionary.txt'](conversion dictionary.txt) is a python dictionary where every word in the training corpus is assigned a number.\n",
    "\n",
    "Then, we generate the neural network and start training. The generated model is stored in a folder named 'tensorflow/logdir\n",
    "\n",
    "Finally, we use the stored model to predict on new data. Results stored in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run script to build vocabulary from training data \n",
    "\n",
    "exec(open('numberizer.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample length: 194\n",
      "Vocabulary size: 15139 words\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../tensorflow/logdir', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f71e0cb26d8>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../tensorflow/logdir/cv_run_1', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7220501240>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "Running 5 fold cross-validation run 1 out of 5.\n",
      "{0: [8, 5], 1: [28, 20], 2: [35, 30, 31, 37]}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ../tensorflow/logdir/cv_run_1/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1052934, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ../tensorflow/logdir/cv_run_1/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.17381069.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../tensorflow/logdir/cv_run_1/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Testing size: 3388, Training size 10507\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-06-28-19:56:00\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../tensorflow/logdir/cv_run_1/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-06-28-19:56:00\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.82762694, global_step = 100, loss = 0.49242666\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: ../tensorflow/logdir/cv_run_1/model.ckpt-100\n",
      "Accuracy (tensorflow): 0.827627\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../tensorflow/logdir/cv_run_2', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f719ce99b70>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "Running 5 fold cross-validation run 2 out of 5.\n",
      "{0: [8, 5, 0, 9], 1: [28, 20, 15, 26], 2: [35, 30, 31, 37, 34, 32, 36, 39]}\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Saving checkpoints for 0 into ../tensorflow/logdir/cv_run_2/model.ckpt.\n",
      "INFO:tensorflow:loss = 1.1253694, step = 1\n",
      "INFO:tensorflow:Saving checkpoints for 100 into ../tensorflow/logdir/cv_run_2/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 0.1763723.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../tensorflow/logdir/cv_run_2/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "Testing size: 3805, Training size 10090\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Starting evaluation at 2020-06-28-19:57:51\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from ../tensorflow/logdir/cv_run_2/model.ckpt-100\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:Finished evaluation at 2020-06-28-19:57:51\n",
      "INFO:tensorflow:Saving dict for global step 100: accuracy = 0.7156373, global_step = 100, loss = 0.660932\n",
      "INFO:tensorflow:Saving 'checkpoint_path' summary for global step 100: ../tensorflow/logdir/cv_run_2/model.ckpt-100\n",
      "Accuracy (tensorflow): 0.715637\n",
      "INFO:tensorflow:Using default config.\n",
      "INFO:tensorflow:Using config: {'_model_dir': '../tensorflow/logdir/cv_run_3', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': None, '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f71931d5f28>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "Running 5 fold cross-validation run 3 out of 5.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot take a larger sample than population when 'replace=False'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-1bcd1b83deaf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Run this script to start training the neural network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TF_classification_BW.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mrun_model\u001b[0;34m(model_dir, mode, test_franction, fold)\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mcreate_test_set\u001b[0;34m(blocks, labels, source, mode, fold, forbidden, test_fraction)\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot take a larger sample than population when 'replace=False'"
     ]
    }
   ],
   "source": [
    "# Run this script to start training the neural network\n",
    "\n",
    "exec(open('TF_classification_BW.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from \"../tensorflow/logdir/metadata.txt\"\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: ../tensorflow/logdir/metadata.txt/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-a2faef357148>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Run this script to predict on unseen data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#Predicts the class of blocks in 'Unlabeled_data' table in the climate.db database.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TF_classification_predict.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_directory)\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(sess, tags, export_dir, **saver_kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;31m# Build the SavedModel protocol buffer and find requested meta graph def.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m     \u001b[0msaved_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m     \u001b[0mfound_match\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mmeta_graph_def\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msaved_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/srv/conda/envs/notebook/lib/python3.6/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36m_parse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     77\u001b[0m                   (export_dir,\n\u001b[1;32m     78\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ../tensorflow/logdir/metadata.txt/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "#Run this script to predict on unseen data\n",
    "#Predicts the class of blocks in 'Unlabeled_data' table in the climate.db database.\n",
    "exec(open('TF_classification_predict.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is finished and you should have obtained probabilities and class predictions for your input data in \"Mixed policy documents\" folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
