{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pilot Project: Learning the Climate\n",
    "\n",
    "Author: Shashi Badloe, Ioannis N. Athanasiadis\n",
    "\n",
    "\n",
    "Purpose: Provides a step-by-step interactive pipeline for the cleanup, modeling and storing of data used in paper:  \n",
    "Robbert Biesbroek, Shashi Badloe, Ioannis N. Athanasiadis (2020). [Machine learning for research on climate change adaptation policy integration: an exploratory UK case study](http://dx.doi.org/10.1007/s10113-020-01677-8), published in Regional Environmental Change.\n",
    "\n",
    "\n",
    "Note: It is important to not change any folder names or directories created during intermediate steps in the pipeline. Doing so, may raise errors as the scripts look in folders with these specific names in specified relative directories.\n",
    "\n",
    "\n",
    "For further details on the function of each script, please look into the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Defining input data\n",
    "To start the analysis, we need to define the training data and testing data. In the folder 'PDF_files' the PDF documents used as training or testing data are contained. \n",
    "\n",
    "The following four folders should be in `PDF_files`:\n",
    "- `Adaptation policy documents`: Training data for adaptation policies\n",
    "- `Mitigation policy documents`: Training data for mitigation policies\n",
    "- `Non-climate policy documents`: Training data for non-climate documents\n",
    "- `Mixed policy documents`: Testing data, any PDF document(s) you want to predict on.\n",
    "\n",
    "You may define your own training and testing data by placing the PDF files in the specified folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Turning PDF into raw text and translating into bag-of-words\n",
    "PDF files are inaccessible to the machine learning algorithm, therefore we need to extract the raw text from PDF files. Converting from PDF to raw text can alter/break the composition of the text (i.e. add invalid characters or whitespace between words). We have applied some automated checks and edits to fix the most common problems, but there may still be  files that do not translate into raw text properly.\n",
    "\n",
    "The following scripts are used to convert and clean the PDF files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set working directory to the folder that holds all scripts\n",
    "import os\n",
    "script_folder = 'Python Scripts'\n",
    "if not os.getcwd().endswith(script_folder):\n",
    "    os.chdir(script_folder)\n",
    "print(\"Current Working Directory \" , os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run script to convert PDF into raw text\n",
    "#Attempts to preserve paragraph structure within text and fixes for invalid characters.\n",
    "#This connects to a server for PDF parsing so internet connection is required. If connection fails try re-running this.\n",
    "#Outputs folder named parsed_files that holds raw text for every PDF file\n",
    "exec(open('pdf_parser.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run script to clean up text and create eligible blocks based on paragraph size.\n",
    "#Uses tagger to determine word types and filters for useful words\n",
    "#Outputs folder named structured_files that contains a bag-of-words for every file in python list format\n",
    "exec(open('text_cleanup.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Building the database\n",
    "A SQLite database is created to hold every block. This database allows for quick storing and retrieval of data and is required when working with big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run script to create database and lots training data in Labeled_data and testing data in Unlabeled_data tables.\n",
    "#Outputs 'climate.db' file in scripts folder. Will overwrite any file with the same name.\n",
    "\n",
    "#Note that this requires a supply of metadata in 'metadata.txt'\n",
    "#The metadata should have a python dictionary format where the key is the filename and contents are a \n",
    "#tuple of date and department like so: \"pdf_filename.pdf: ('day month year', 'Department')\"\n",
    "exec(open('sqlite_db.py').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Training the model\n",
    "The model is a simple feed forward neural network model. The input are the blocks and it assigns/adjusts weights of each word towards the three classes depending on how often they occur in the training data for that class or how often they co-occur in the same bag as a word strongly correlated to a class.\n",
    "\n",
    "\n",
    "'TF_classification_predict.py' - Uses stored model to predict on new data. Results stored in database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run script to build vocabulary from training data \n",
    "#Output file 'conversion disctionary.txt' is a python dictionary where every word in the training\n",
    "#corpus is assigned a number\n",
    "exec(open('numberizer.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this script to start training the neural network\n",
    "#Internal validation with cross validation or regular split is possible\n",
    "#Outputs a folder named 'tensorflow/logdir' in the root folder containing the model\n",
    "(open('TF_classification_BW.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from numberizer import connect_to_db\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import os\n",
    "import pandas\n",
    "import time\n",
    "import TF_classification_BW as m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_column = tf.feature_column.categorical_column_with_identity(WORDS_FEATURE, num_buckets=n_words)\n",
    "bow_embedding_column = tf.feature_column.embedding_column(bow_column, dimension=EMBEDDING_SIZE)\n",
    "bow = tf.feature_column.input_layer(features, feature_columns=[bow_embedding_column])\n",
    "logits = tf.layers.dense(bow, MAX_LABEL, activation=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import TF_classification_predict as m2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_directory = '../tensorflow/logdir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported = tf.saved_model.load(\"../tensorflow/logdir/1563368487/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported.initializer.resource_handle()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imported.graph.get_collection('trainable_varaibles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this script to predict on unseen data\n",
    "#Predicts the class of blocks in 'Unlabeled_data' table in the climate.db database.\n",
    "exec(open('TF_classification_predict.py').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model('../tensorflow/logdir/1563368487/')\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pipeline is finished and you should have obtained probabilities and class predictions for your input data in \"Mixed policy documents\" folder."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
